---
title: "Milestone Report"
author: "Ben Kling"
date: "5/30/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats)
library(readtext)
library(gt)
```

# Milestone Report - Text Prediction

In this report, I will outline major features of three different input files, 
twitter posts, blog posts, and news articles and then provide next steps for
this project, that include creating the predictive model as well as packaging
the model into a Shiny App.

## Data Load and basic features
```{r load full files, warning=FALSE, cache=TRUE}
# if the files do not exist, download them
if(! file.exists("Coursera-Swiftkey.zip")) {
    download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", 
                  destfile="Coursera-Swiftkey.zip")
}

# if the unzipped files do not exist, unzip them
if(! file.exists("final/en_US/en_US.twitter.txt")) { 
    unzip("Coursera-Swiftkey.zip")
}

# read the data
twitter_data <- read_lines("final/en_US/en_US.twitter.txt")
blogs_data <- read_lines("final/en_US/en_US.blogs.txt")
news_data <- read_lines("final/en_US/en_US.news.txt")

```



The data are very large, totalling **`r 267.8 + 269.8 + 334.5 `** MiB and
**`r length(twitter_data) + length(blogs_data) + length(news_data)`** records of
data. In order to perform any intensive calculations against this data we will
need to sample the data. At this point it feels sufficient to simply retrieve a
5% sample of the data, though this may be changed in the model creation process
if we are unable to produce an accurate prediction.

Before sampling, let's look at a few summary statistics to better understand
each of these data sets:
```{r full file summary stats}

summary_stats <- tibble(source = "twitter",
                        records = length(twitter_data),
                        size_mib = object.size(twitter_data) / 1000.0 / 1000.0,
                        min_length = min(nchar(twitter_data)),
                        max_length = max(nchar(twitter_data)),
                        mean_length = mean(nchar(twitter_data))) %>%
    bind_rows(tibble(source = "blogs",
                        records = length(blogs_data),
                        size_mib = object.size(blogs_data) / 1000.0 / 1000.0,
                        min_length = min(nchar(blogs_data)),
                        max_length = max(nchar(blogs_data)),
                        mean_length = mean(nchar(blogs_data)))) %>%
    bind_rows(tibble(source = "news",
                        records = length(news_data),
                        size_mib = object.size(news_data) / 1000.0 / 1000.0,
                        min_length = min(nchar(news_data)),
                        max_length = max(nchar(news_data)),
                        mean_length = mean(nchar(news_data))))

gt(summary_stats) %>%
    tab_header(
        title = "Summary Statistics",
        subtitle = md("*(length presented as number of characters)*")
    )

```

From this we can see that:

- As expected, twitter records ("tweets") have a maximium of 140 total characters
per entry, and average 69 characters per tweet

- the majority of our records sample comes from twitter, and despite being 
shorter in average length, contributes more overall data to the body of text
that we are looking at. 

- Blogs and news articles are both more similar to each other, containing an 
average of 200-230 characters per entry, but ranging to over 40k characters
for the longest blog post.


Next, we will begin to analyze the actual words contained in the report. For all 
of our exploratory work, we'll be using the 5% sample of the data to ensure 
processing efficiency.

```{r sample files}
# retrieve a 5% sample of the data
set.seed(1066) # set the seed for reproducibility
sample_size <- 0.05
twitter_sample <- sample(twitter_data, 
                         size = as.integer(length(twitter_data) * sample_size))
blogs_sample <- sample(blogs_data,
                       size = as.integer(length(blogs_data) * sample_size))
news_sample <- sample(news_data,
                      size = as.integer(length(news_data) * sample_size))


# write the sample data back out
write_lines(twitter_sample, "final/en_US/twittersample.txt")
write_lines(blogs_sample, "final/en_US/blogsample.txt")
write_lines(news_sample, "final/en_US/newssample.txt")
```

```{r load the sampled files}
# file path variables
us_path <- "final/en_US/"
twitter_file <- "twittersample.txt"
blog_file <- "blogsample.txt"
news_file <- "newssample.txt"

# read the text
twitter_sample <- read_lines(paste0(us_path, twitter_file))
blog_sample <- read_lines(paste0(us_path, blog_file))
news_sample <- read_lines(paste0(us_path, news_file))


# make a data frame of all of these different texts
df <- tibble("source" = "twitter",
                 "text" = twitter_sample)
df <- bind_rows(df, tibble("source" = "blog",
                               "text" = blog_sample))
df <- bind_rows(df, tibble("source" = "news",
                               "text" = news_sample))


# create corpora
cp <- corpus(df, text_field = "text")


# create tokens
tk <- tokens(cp,
             remove_punct = T,
             remove_symbols = T,
             remove_numbers = F,
             remove_url = T,
             verbose = T)

```
