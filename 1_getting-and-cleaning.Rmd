---
title: "getting-and-cleaning"
author: "Ben Kling"
date: "5/23/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats)
library(readtext)
```

In this doc, we'll load up some of the US files and start to examine them

### Functions that need to be written

1. Tokenization - given a file as input, return a tokenized version of the file
2. Profanity Filtering - removing profanity and other words we do not want to 
predict

## Load the full file, produce sample files
```{r load}

# read the data
twitter_data <- read_lines("final/en_US/en_US.twitter.txt")
blogs_data <- read_lines("final/en_US/en_US.blogs.txt")
news_data <- read_lines("final/en_US/en_US.news.txt")

# retrieve a 25% sample of the data
set.seed(1066) # set the seed for reproducibility
twitter_sample <- sample(twitter_data, 
                         size = as.integer(length(twitter_data) * 0.25))
blogs_sample <- sample(blogs_data,
                       size = as.integer(length(blogs_data) * 0.25))
news_sample <- sample(news_data,
                      size = as.integer(length(news_data) * 0.25))


# write the sample data back out
write_lines(twitter_sample, "final/en_US/twittersample.txt")
write_lines(blogs_sample, "final/en_US/blogsample.txt")
write_lines(news_sample, "final/en_US/newssample.txt")
```



```{r for quiz}
# The en_US.twitter.txt has  how many lines of text?
length(twitter_data)

# longest line
max(nchar(twitter_data))
max(nchar(blogs_data))
max(nchar(news_data))

# love /hate
loves <- sum(grepl("love", twitter_data))
hates <- sum(grepl("hate", twitter_data))
loves / hates

# biostats tweet
twitter_data[grep("biostats", twitter_data)]

# match count for string
length(grep("A computer once beat me at chess, but it was no match for me at kickboxing",
            twitter_data))
```



```{r sample files in quanteda}
# file path variables
us_path <- "final/en_US/"
twitter_file <- "twittersample.txt"
blog_file <- "blogsample.txt"
news_file <- "newssample.txt"

# read the text
twitter_sample <- read_lines(paste0(us_path, twitter_file))
blog_sample <- read_lines(paste0(us_path, blog_file))
news_sample <- read_lines(paste0(us_path, news_file))


# make a data frame of all of these different texts
df <- data.table("source" = "twitter",
                 "text" = twitter_sample)
df <- bind_rows(df, data.table("source" = "blog",
                               "text" = blog_sample))
df <- bind_rows(df, data.table("source" = "news",
                               "text" = news_sample))

# summarize data
df %>%
    group_by(source) %>%
    summarize(entries = n())

# if you need to set options
opt <- quanteda_options()



# create corpora
cp <- corpus(df, text_field = "text")


# create tokens
tk <- tokens(cp,
             remove_punct = T,
             remove_symbols = T,
             remove_numbers = T,
             remove_url = T,
             verbose = T)



# remove stopwords
# twitter_tokens <- tokens_select(twitter_tokens,
#                                 pattern = stopwords("en"),
#                                 selection = "remove")



# ngrams
ngrm <- tokens_ngrams(tk, n=2:3)



# document term matrix
twitter_dfm <- dfm(twitter_tokens, tolower=T)
head(twitter_ngrams[[1]],20)



# EDA
textplot_wordcloud(twitter_dfm, max_words=50)
k <- kwic(tk, "are", window = 5)
head(k, 10)

```
