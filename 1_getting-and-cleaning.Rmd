---
title: "getting-and-cleaning"
author: "Ben Kling"
date: "5/23/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats)
library(readtext)
```

In this doc, we'll load up some of the US files and start to examine them

### Functions that need to be written

1. Tokenization - given a file as input, return a tokenized version of the file
2. Profanity Filtering - removing profanity and other words we do not want to 
predict

## Load the full file, produce sample files
```{r load}
# if the files do not exist, download them
if(! file.exists("Coursera-Swiftkey.zip")) {
    download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", 
                  destfile="Coursera-Swiftkey.zip")
}

if(! file.exists("final/en_US/en_US.twitter.txt")) { 
    unzip("Coursera-Swiftkey.zip")
}

# read the data
twitter_data <- read_lines("final/en_US/en_US.twitter.txt")
blogs_data <- read_lines("final/en_US/en_US.blogs.txt")
news_data <- read_lines("final/en_US/en_US.news.txt")

# retrieve a 25% sample of the data
set.seed(1066) # set the seed for reproducibility
twitter_sample <- sample(twitter_data, 
                         size = as.integer(length(twitter_data) * 0.25))
blogs_sample <- sample(blogs_data,
                       size = as.integer(length(blogs_data) * 0.25))
news_sample <- sample(news_data,
                      size = as.integer(length(news_data) * 0.25))


# write the sample data back out
write_lines(twitter_sample, "final/en_US/twittersample.txt")
write_lines(blogs_sample, "final/en_US/blogsample.txt")
write_lines(news_sample, "final/en_US/newssample.txt")
```



```{r for quiz}
# The en_US.twitter.txt has  how many lines of text?
length(twitter_data)

# longest line
max(nchar(twitter_data))
max(nchar(blogs_data))
max(nchar(news_data))

# love /hate
loves <- sum(grepl("love", twitter_data))
hates <- sum(grepl("hate", twitter_data))
loves / hates

# biostats tweet
twitter_data[grep("biostats", twitter_data)]

# match count for string
length(grep("A computer once beat me at chess, but it was no match for me at kickboxing",
            twitter_data))
```


```{r quanteda options}

# if you need to set options
opt <- quanteda_options()


```



```{r sample files in quanteda}
# file path variables
us_path <- "final/en_US/"
twitter_file <- "twittersample.txt"
blog_file <- "blogsample.txt"
news_file <- "newssample.txt"

# read the text
twitter_sample <- read_lines(paste0(us_path, twitter_file))
blog_sample <- read_lines(paste0(us_path, blog_file))
news_sample <- read_lines(paste0(us_path, news_file))


# make a data frame of all of these different texts
df <- tibble("source" = "twitter",
                 "text" = twitter_sample)
df <- bind_rows(df, tibble("source" = "blog",
                               "text" = blog_sample))
df <- bind_rows(df, tibble("source" = "news",
                               "text" = news_sample))


# create corpora
cp <- corpus(df, text_field = "text")


# create tokens
tk <- tokens(cp,
             remove_punct = T,
             remove_symbols = T,
             remove_numbers = F,
             remove_url = T,
             verbose = T)

```

```{r token modification}

# replace all numbers with a placeholder
tk_comp <- tk %>%
    tokens_replace(pattern = "\\d+",
                   valuetype = "regex",
                   replacement = "**NUMBER**")

tk_comp[[1]]
```



```{r quick qa checks}

# summarize data
df %>%
    group_by(source) %>%
    summarize(entries = n())



```

# remove stopwords
nostop_tokens <- tokens_select(tk,
                                pattern = stopwords("en"),
                                selection = "remove")

tokens_

# ngrams
ngrm <- tokens_ngrams(tk, n=2)
head(ngrm[[1]],20)


# document term matrix
docmatrix <- dfm(tk, tolower=T)
dm_nostop <- dfm(nostop_tokens, tolower = T)

#grouped document feature matrix
dmg <- dfm_group(docmatrix, groups = source)
dmg_nostop <- dfm_group(dm_nostop, groups = source)



#keyness object
tstat_key <- textstat_keyness(dmg_nostop, 
                              target = dmg_nostop$source == 'twitter')



# visualizations
textplot_wordcloud(dmg_nostop, max_words=50, comparison = T)
textplot_keyness(tstat_key)

# keyword in context
k <- kwic(tk, "are", window = 5)
head(k, 10)

```
