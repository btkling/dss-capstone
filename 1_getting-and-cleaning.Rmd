---
title: "getting-and-cleaning"
author: "Ben Kling"
date: "5/23/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tm)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats)
library(readtext)
```

In this doc, we'll load up some of the US files and start to examine them

### Functions that need to be written

1. Tokenization - given a file as input, return a tokenized version of the file
2. Profanity Filtering - removing profanity and other words we do not want to 
predict

## Load the full file, produce sample files
```{r load}

# read the data
twitter_data <- read_lines("final/en_US/en_US.twitter.txt")
blogs_data <- read_lines("final/en_US/en_US.blogs.txt")
news_data <- read_lines("final/en_US/en_US.news.txt")

# retrieve a 25% sample of the data
set.seed(1066) # set the seed for reproducibility
twitter_sample <- sample(twitter_data, 
                         size = as.integer(length(twitter_data) * 0.25))
blogs_sample <- sample(blogs_data,
                       size = as.integer(length(blogs_data) * 0.25))
news_sample <- sample(news_data,
                      size = as.integer(length(news_data) * 0.25))


# write the sample data back out
write_lines(twitter_sample, "final/en_US/twittersample.txt")
write_lines(blogs_sample, "final/en_US/blogsample.txt")
write_lines(news_sample, "final/en_US/newssample.txt")
```


```{r for quiz}
# The en_US.twitter.txt has  how many lines of text?
length(twitter_data)

# longest line
max(nchar(twitter_data))
max(nchar(blogs_data))
max(nchar(news_data))

# love /hate
loves <- sum(grepl("love", twitter_data))
hates <- sum(grepl("hate", twitter_data))
loves / hates

# biostats tweet
twitter_data[grep("biostats", twitter_data)]

# match count for string
length(grep("A computer once beat me at chess, but it was no match for me at kickboxing",
            twitter_data))
```

```{r sample files in quanteda}

# file path variables
us_path <- "final/en_US/"
twitter_file <- "twittersample.txt"
blog_file <- "blogsample.txt"
news_file <- "newssample.txt"

# read the text

twitter_sample <- read_lines(paste0(us_path, twitter_file))
blog_sample <- read_lines(paste0(us_path, blog_file))
news_sample <- read_lines(paste0(us_path, news_file))

opt <- quanteda_options()

# create corpora
twitter_cp <- corpus(twitter_sample)
blog_cp <- corpus(blog_sample)
news_cp <- corpus(news_sample)


twitter_dtm <- dfm(twitter_cp, 
                   remove = stopwords("en"),
                   remove_punct = T)
twitter_tokens <- tokens(twitter_cp,
                         remove_punct = T,
                         remove_symbols = T,
                         remove_numbers = T,
                         remove_url = T,
                         verbose = T)
twitter_tokens <- tokens_select(twitter_tokens,
                                pattern = stopwords("en"),
                                selection = "remove")

twitter_ngrams <- tokens_ngrams(twitter_tokens, n=2:3)
twitter_dfm <- dfm(twitter_tokens)
head(twitter_ngrams[[1]],20)

textplot_wordcloud(twitter_dfm, max_words=50)
textstat_collocations(twitter_tokens)

```
