---
title: "getting-and-cleaning"
author: "Ben Kling"
date: "5/23/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats)
library(readtext)
```

In this doc, we'll load up some of the US files and start to examine them

### Functions that need to be written

1. Tokenization - given a file as input, return a tokenized version of the file
2. Profanity Filtering - removing profanity and other words we do not want to 
predict

## Load the full file, produce sample files
```{r load}
# if the files do not exist, download them
if(! file.exists("Coursera-Swiftkey.zip")) {
    download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", 
                  destfile="Coursera-Swiftkey.zip")
}

if(! file.exists("final/en_US/en_US.twitter.txt")) { 
    unzip("Coursera-Swiftkey.zip")
}

# read the data
twitter_data <- read_lines("final/en_US/en_US.twitter.txt")
blogs_data <- read_lines("final/en_US/en_US.blogs.txt")
news_data <- read_lines("final/en_US/en_US.news.txt")

# retrieve a 25% sample of the data
set.seed(1066) # set the seed for reproducibility
sample_size <- 0.05
twitter_sample <- sample(twitter_data, 
                         size = as.integer(length(twitter_data) * sample_size))
blogs_sample <- sample(blogs_data,
                       size = as.integer(length(blogs_data) * sample_size))
news_sample <- sample(news_data,
                      size = as.integer(length(news_data) * sample_size))


# write the sample data back out
write_lines(twitter_sample, "final/en_US/twittersample.txt")
write_lines(blogs_sample, "final/en_US/blogsample.txt")
write_lines(news_sample, "final/en_US/newssample.txt")
```



```{r for quiz}
# The en_US.twitter.txt has  how many lines of text?
length(twitter_data)

# longest line
max(nchar(twitter_data))
max(nchar(blogs_data))
max(nchar(news_data))

# love /hate
loves <- sum(grepl("love", twitter_data))
hates <- sum(grepl("hate", twitter_data))
loves / hates

# biostats tweet
twitter_data[grep("biostats", twitter_data)]

# match count for string
length(grep("A computer once beat me at chess, but it was no match for me at kickboxing",
            twitter_data))
```



```{r quanteda options}

# if you need to set options
opt <- quanteda_options()


```



```{r sample files in quanteda}
# file path variables
us_path <- "final/en_US/"
twitter_file <- "twittersample.txt"
blog_file <- "blogsample.txt"
news_file <- "newssample.txt"

# read the text
twitter_sample <- read_lines(paste0(us_path, twitter_file))
blog_sample <- read_lines(paste0(us_path, blog_file))
news_sample <- read_lines(paste0(us_path, news_file))


# make a data frame of all of these different texts
df <- tibble("source" = "twitter",
                 "text" = twitter_sample)
df <- bind_rows(df, tibble("source" = "blog",
                               "text" = blog_sample))
df <- bind_rows(df, tibble("source" = "news",
                               "text" = news_sample))


# create corpora
cp <- corpus(df, text_field = "text")


# create tokens
tk <- tokens(cp,
             remove_punct = T,
             remove_symbols = T,
             remove_numbers = F,
             remove_url = T,
             verbose = T)

```



```{r token modification}

# replace all numbers with a placeholder
tk_clean <- tk %>%
    tokens_replace(pattern = "\\d+",
                   valuetype = "regex",
                   replacement = "**NUMBER**")

# check one entry of tokens
tk_comp[[1]]
```



```{r quick qa checks}

# summarize data

# Number of entries
df %>%
    group_by(source) %>%
    summarize(entries = n())



```



```{r analysis and visualizations}
# remove stopwords
nostop_tokens <- tokens_select(tk,
                                pattern = stopwords("en"),
                                selection = "remove")

# ngrams
ngrm <- tokens_ngrams(nostop_tokens, n=2)
trgm <- tokens_ngrams(nostop_tokens, n=3)
head(ngrm[[1]],20)

# document feature matrix
dm_full <- dfm(tk, tolower=T)
dm_nostop <- dfm(nostop_tokens, tolower = T)
dm_bigrm <- dfm(ngrm, tolower = T)
dm_trigrm <- dfm(trgm, tolower = T)

#grouped document feature matrix
dmg <- dfm_group(docmatrix, groups = source)
dmg_nostop <- dfm_group(dm_nostop, groups = source)

#keyness object
tstat_key <- textstat_keyness(dmg_nostop, 
                              target = dmg_nostop$source == 'twitter')

# visualizations
textplot_wordcloud(dmg_nostop, max_words=50, comparison = T)
textplot_keyness(tstat_key)

# plot the 15 most commonly occurring words
dm_nostop %>% 
    textstat_frequency(n = 15) %>%
    head()


dm_nostop %>% 
  textstat_frequency(n = 15) %>% 
  ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
  geom_bar(stat='identity') +
  coord_flip() +
  labs(x = NULL, y = "Frequency") +
  theme_minimal()

dm_bigrm %>%
    textstat_frequency(n = 15) %>%
    ggplot(aes(x = reorder(feature, frequency), y = frequency)) + 
    geom_bar(stat='identity') + 
    coord_flip() + 
    labs(x= NULL, y = 'Frequency', title = "2-Gram Frequency") + 
    theme_minimal()

dm_trigrm %>% 
    textstat_frequency(n = 15) %>%
    ggplot(aes(x = reorder(feature, frequency), y = frequency)) + 
    geom_bar(stat='identity') + 
    coord_flip() + 
    labs(x= NULL, y = 'Frequency', title = "3-Gram Frequency") + 
    theme_minimal()

# keyword in context
k <- kwic(tk, "are", window = 5)
head(k, 10)

```
